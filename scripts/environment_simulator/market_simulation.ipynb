{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 19:47:01,854\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import django\n",
    "import os\n",
    "file_dir = \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"\n",
    "if file_dir not in sys.path:\n",
    "    sys.path.insert(0, file_dir)\n",
    "\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"commissionerv2.settings\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\" \n",
    "django.setup()\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "from django.db.models import Q\n",
    "from apps.environment_simulator.sevice_layer.market_simulator import MyEnv\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from gymnasium import spaces\n",
    "from ray.rllib.algorithms import ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 19:47:07,119\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-10-05 19:47:07,482\tINFO packaging.py:518 -- Creating a file package for local directory '/Users/mirbilal/Desktop/MobCommission/commissionV2/'.\n",
      "2023-10-05 19:47:07,724\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_2745a9edf67ca284.zip' (6.74MiB) to Ray cluster...\n",
      "2023-10-05 19:47:07,755\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_2745a9edf67ca284.zip'.\n",
      "2023-10-05 19:47:09,249\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-05 19:47:09,250\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class 'apps.environment_simulator.sevice_layer.market_simulator.MyEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'apps.environment_simulator.sevice_layer.market_simulator.MyEnv'>').build()` instead. This will raise an error in the future!\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=44566)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-10-05 19:47:20,158\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-05 19:47:20,168\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-05 19:47:20,169\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-05 19:47:20,170\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-05 19:47:20,170\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44566)\u001b[0m /Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44566)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44566)\u001b[0m /Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44566)\u001b[0m   logger.deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44566)\u001b[0m 2023-10-05 19:47:20,125\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,122\tWARNING utils.py:152 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,123\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,135\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,135\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,136\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=44563)\u001b[0m 2023-10-05 19:47:20,136\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-05 19:47:20,607\tINFO trainable.py:188 -- Trainable.setup took 11.310 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(runtime_env={\"working_dir\": \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"})\n",
    "algo = ppo.PPO(env=MyEnv,config={\"num_workers\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward mean: 0 -0.991\n",
      "episode reward mean: 1 -0.991\n",
      "episode reward mean: 2 -0.991\n",
      "episode reward mean: 3 -0.9845\n",
      "episode reward mean: 4 -0.975\n",
      "episode reward mean: 5 -0.969\n",
      "episode reward mean: 6 -0.9535\n",
      "episode reward mean: 7 -0.941\n",
      "episode reward mean: 8 -0.925\n",
      "episode reward mean: 9 -0.9155\n",
      "episode reward mean: 10 -0.8895\n",
      "episode reward mean: 11 -0.871\n",
      "episode reward mean: 12 -0.8525\n",
      "episode reward mean: 13 -0.839\n",
      "episode reward mean: 14 -0.83\n",
      "episode reward mean: 15 -0.7985\n",
      "episode reward mean: 16 -0.7765\n",
      "episode reward mean: 17 -0.763\n",
      "episode reward mean: 18 -0.732\n",
      "episode reward mean: 19 -0.6855\n",
      "episode reward mean: 20 -0.6535\n",
      "episode reward mean: 21 -0.612\n",
      "episode reward mean: 22 -0.5745\n",
      "episode reward mean: 23 -0.5165\n",
      "episode reward mean: 24 -0.466\n",
      "episode reward mean: 25 -0.4075\n",
      "episode reward mean: 26 -0.314\n",
      "episode reward mean: 27 -0.205\n",
      "episode reward mean: 28 -0.0895\n",
      "episode reward mean: 29 0.002\n",
      "episode reward mean: 30 0.0945\n",
      "episode reward mean: 31 0.2065\n",
      "episode reward mean: 32 0.3075\n",
      "episode reward mean: 33 0.403\n",
      "episode reward mean: 34 0.538\n",
      "episode reward mean: 35 0.6285\n",
      "episode reward mean: 36 0.6785\n",
      "episode reward mean: 37 0.7575\n",
      "episode reward mean: 38 0.8075\n",
      "episode reward mean: 39 0.819\n",
      "episode reward mean: 40 0.869\n",
      "episode reward mean: 41 0.881\n",
      "episode reward mean: 42 0.9275\n",
      "episode reward mean: 43 0.95\n",
      "episode reward mean: 44 0.9515\n",
      "episode reward mean: 45 0.975\n",
      "episode reward mean: 46 0.9775\n",
      "episode reward mean: 47 0.9865\n",
      "episode reward mean: 48 0.9915\n",
      "episode reward mean: 49 0.989\n"
     ]
    }
   ],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(50):\n",
    "\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 2. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/spaces/box.py:130: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MyEnv()\n",
    "obs = MyEnv()\n",
    "obs.reset()\n",
    "\n",
    "latest_action = algo.compute_single_action(0)\n",
    "print(latest_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2env",
   "language": "python",
   "name": "v2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
