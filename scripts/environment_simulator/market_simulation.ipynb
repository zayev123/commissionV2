{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import django\n",
    "import os\n",
    "file_dir = \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"\n",
    "if file_dir not in sys.path:\n",
    "    sys.path.insert(0, file_dir)\n",
    "\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"commissionerv2.settings\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\" \n",
    "django.setup()\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "from django.db.models import Q\n",
    "from apps.environment_simulator.sevice_layer.market_simulator import MarketSimulator\n",
    "from apps.environment_simulator.models import SimulatedStockBuffer\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from gymnasium import spaces\n",
    "from ray.rllib.algorithms import ppo\n",
    "from os import path, environ\n",
    "from gymnasium.envs.registration import register\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "    'database': environ.get(\"POSTGRES_DB\"),\n",
    "    'user': environ.get(\"POSTGRES_USER\"),\n",
    "    'password': environ.get(\"POSTGRES_PASSWORD\"),\n",
    "    'host': environ.get(\"DB_HOST\"),\n",
    "    'port': environ.get(\"DB_PORT\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 19:53:20,574\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-10-08 19:53:21,027\tINFO packaging.py:518 -- Creating a file package for local directory '/Users/mirbilal/Desktop/MobCommission/commissionV2/'.\n",
      "2023-10-08 19:53:21,316\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_b18693a824f40c39.zip' (8.25MiB) to Ray cluster...\n",
      "2023-10-08 19:53:21,355\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_b18693a824f40c39.zip'.\n",
      "2023-10-08 19:53:24,363\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29515)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,718\tWARNING utils.py:152 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m /Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/wrappers/compatibility.py:60: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v1.0. Instead use `gymnasium.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m   logger.deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,758\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29517)\u001b[0m 2023-10-08 19:53:38,847\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 19:53:38,908\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 19:53:38,932\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 19:53:38,933\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 19:53:38,933\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 19:53:38,934\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,868\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,868\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,869\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29515)\u001b[0m 2023-10-08 19:53:38,869\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 19:53:39,548\tINFO trainable.py:188 -- Trainable.setup took 15.142 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(runtime_env={\"working_dir\": \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"})\n",
    "\n",
    "the_current_time_step  = datetime(year=1995, month=1, day=1, hour=10)\n",
    "\n",
    "config = {\n",
    "        \"env\": MarketSimulator,\n",
    "        \"env_config\": {\n",
    "            \"db_params\": db_params, \n",
    "            \"max_episode_steps\": 150, \n",
    "            \"the_current_time_step\": the_current_time_step,\n",
    "            \"print_output\": False\n",
    "        },\n",
    "        \"num_workers\": 4,\n",
    "        \"entropy_coeff\": 0.5,\n",
    "        \"framework_str\": \"tf2\"\n",
    "}\n",
    "\n",
    "algo = ppo.PPO(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-08 20:08:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:34.62        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.9/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MarketSimulator_6d425_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/logs/PPO_2023-10-08_20-08-20/PPO_MarketSimulator_6d425_00000_0_2023-10-08_20-08-20/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MarketSimulator_6d425_00000</td><td>ERROR   </td><td>127.0.0.1:31018</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 20:08:20,365\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=31018)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:34,093\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:34,094\tWARNING algorithm_config.py:672 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=31040)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31041)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31040)\u001b[0m /Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/wrappers/compatibility.py:60: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v1.0. Instead use `gymnasium.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31040)\u001b[0m   logger.deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 2023-10-08 20:08:46,199\tWARNING utils.py:152 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31039)\u001b[0m 2023-10-08 20:08:46,225\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:46,337\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:46,353\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:46,353\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:46,353\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m 2023-10-08 20:08:46,353\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=31018)\u001b[0m Trainable.setup took 12.338 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-10-08 20:08:54,966\tERROR tune_controller.py:1502 -- Trial task failed for trial PPO_MarketSimulator_6d425_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2547, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::PPO.train()\u001b[39m (pid=31018, ip=127.0.0.1, actor_id=a2b94f2086213b84d9bd4c5201000000, repr=PPO)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 400, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 397, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2838, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 448, in training_step\n",
      "    train_results = self.learner_group.update(\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/core/learner/learner_group.py\", line 184, in update\n",
      "    self._learner.update(\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/core/learner/learner.py\", line 1304, in update\n",
      "    ) = self._update(nested_tensor_minibatch)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/core/learner/torch/torch_learner.py\", line 365, in _update\n",
      "    return self._possibly_compiled_update(batch)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/core/learner/torch/torch_learner.py\", line 123, in _uncompiled_update\n",
      "    loss_per_module = self.compute_loss(fwd_out=fwd_out, batch=batch)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/core/learner/learner.py\", line 1024, in compute_loss\n",
      "    loss = self.compute_loss_for_module(\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/torch/ppo_torch_learner.py\", line 87, in compute_loss_for_module\n",
      "    action_kl = prev_action_dist.kl(curr_action_dist)\n",
      "  File \"/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/models/torch/torch_distributions.py\", line 327, in kl\n",
      "    for cat, oth_cat in zip(self._cats, other.cats)\n",
      "AttributeError: '<class 'ray.rllib.models.torch.torch_distributions' object has no attribute 'cats'\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_MarketSimulator_6d425_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mean_ppo \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# for _ in range(20):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     result = algo.train()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# for _ in range(20):\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPPO\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtraining_iteration\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     local_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/logs/\u001b[39;49m\u001b[39m\"\u001b[39;49m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# result = analysis.trials[0].last_result\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# # print(\"episode reward mean:\", _, result['episode_reward_mean'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mirbilal/Desktop/MobCommission/commissionV2/scripts/environment_simulator/market_simulation.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# mean_ppo.append(result['episode_reward_mean'])\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/tune.py:1137\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1136\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set():\n\u001b[0;32m-> 1137\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1138\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_MarketSimulator_6d425_00000])"
     ]
    }
   ],
   "source": [
    "# mean_ppo = []\n",
    "# # for _ in range(20):\n",
    "\n",
    "# #     result = algo.train()\n",
    "# #     print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "# #     mean_ppo.append(result['episode_reward_mean'])\n",
    "\n",
    "# # for _ in range(20):\n",
    "# tune.run(\n",
    "#     \"PPO\",\n",
    "#     config=config,\n",
    "#     stop={\"training_iteration\": 1},\n",
    "#     verbose=1,\n",
    "#     local_dir=\"/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/logs/\" \n",
    "# )\n",
    "# # result = analysis.trials[0].last_result\n",
    "# # # print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "# # mean_ppo.append(result['episode_reward_mean'])\n",
    "\n",
    "mean_ppo = []\n",
    "for _ in range(20):\n",
    "\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/simulated_checkpoints/), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 10.009098821611547}, 'default_policy': {'total_loss': 10.009098821611547, 'policy_loss': -0.007832370704409283, 'vf_loss': 10.0, 'vf_loss_unclipped': 218934049.85927504, 'vf_explained_var': -7.670329832064825e-05, 'entropy': 8.125155032570682, 'mean_kl_loss': 0.011148096544407043, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 1.5187500715255737}}, 'num_env_steps_sampled': 280000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 280000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 30548.17571504754, 'episode_reward_min': -12296.816146656987, 'episode_reward_mean': 20259.257581430007, 'episode_len_mean': 151.0, 'episode_media': {}, 'episodes_this_iter': 28, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [23815.9022477549, 13294.095410047936, 26054.470956188423, 24996.130501708103, 24036.629109683854, 23998.01367928742, 30548.17571504754, 2457.2184137478616, 9802.57868411811, 14842.087957322823, 23932.416382487798, 7921.987433962358, 27703.349398182305, 21151.403181349197, 25374.139920822476, 22943.83454430045, 12348.448485261046, 20960.2207046299, 23768.510923259135, 26986.25646078551, 3051.1258346415125, 22835.23398766818, -2488.6544409595444, 24081.252275111277, 17932.786972587048, 23088.82991707835, 23401.022657503956, 24259.631073425717, 24905.98620010273, 11863.10266043269, 18987.36683322834, 20343.177542015263, 24383.770495317818, 25601.641964099283, 12789.950448761869, 11774.189619705867, 24709.314752874037, 24349.51140532502, 25726.26580379581, 26957.26240887616, 2030.7313862140873, 24242.02523766445, 22635.65911988821, 25454.80005213242, 15514.08600273275, 25632.590488547976, 25923.77973580733, 25998.30882873074, 26711.286617513688, 22665.56997151603, 21317.019352467134, 22883.775968058515, 12397.271692792241, 25964.533771035458, 21254.571553840837, 26867.121752406943, 22566.44835214269, 17052.005109301026, 29558.424949944187, 23859.92243636823, 16885.83242040123, 21566.482705160546, 25009.422175607673, 25505.94641922509, 23960.651414591375, 15104.585127433027, 15135.377576007122, 27369.533945860036, 19367.107009941756, 25184.740199809545, 20926.629633714394, 18223.45090641179, 27335.79338117469, 21664.996329838228, 26506.972832494423, 29395.066282011292, 13081.071401895617, 27020.988508753668, 21106.187637654504, 21873.407920322017, 28171.860911665484, 22464.25062227892, 13564.61117203081, 4026.05720184432, 25767.657332662653, 23721.726758150806, 24144.766846303595, 13249.052950870915, 28778.158946633783, 9981.971001673563, 28159.358241689057, -12296.816146656987, 12818.402300538428, 14857.962071743466, 24029.523354879937, 25437.64192021692, -1207.7742953589725, 19813.146072104515, 24914.09569018615, 17287.290460591765], 'episode_lengths': [151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7583810114396217, 'mean_inference_ms': 1.4136270003586466, 'mean_action_processing_ms': 0.1681794029153136, 'mean_env_wait_ms': 2.4441497025023726, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.021897077560424805, 'StateBufferConnector_ms': 0.00515294075012207, 'ViewRequirementAgentConnector_ms': 0.3126258850097656}}, 'episode_reward_max': 30548.17571504754, 'episode_reward_min': -12296.816146656987, 'episode_reward_mean': 20259.257581430007, 'episode_len_mean': 151.0, 'episodes_this_iter': 28, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [23815.9022477549, 13294.095410047936, 26054.470956188423, 24996.130501708103, 24036.629109683854, 23998.01367928742, 30548.17571504754, 2457.2184137478616, 9802.57868411811, 14842.087957322823, 23932.416382487798, 7921.987433962358, 27703.349398182305, 21151.403181349197, 25374.139920822476, 22943.83454430045, 12348.448485261046, 20960.2207046299, 23768.510923259135, 26986.25646078551, 3051.1258346415125, 22835.23398766818, -2488.6544409595444, 24081.252275111277, 17932.786972587048, 23088.82991707835, 23401.022657503956, 24259.631073425717, 24905.98620010273, 11863.10266043269, 18987.36683322834, 20343.177542015263, 24383.770495317818, 25601.641964099283, 12789.950448761869, 11774.189619705867, 24709.314752874037, 24349.51140532502, 25726.26580379581, 26957.26240887616, 2030.7313862140873, 24242.02523766445, 22635.65911988821, 25454.80005213242, 15514.08600273275, 25632.590488547976, 25923.77973580733, 25998.30882873074, 26711.286617513688, 22665.56997151603, 21317.019352467134, 22883.775968058515, 12397.271692792241, 25964.533771035458, 21254.571553840837, 26867.121752406943, 22566.44835214269, 17052.005109301026, 29558.424949944187, 23859.92243636823, 16885.83242040123, 21566.482705160546, 25009.422175607673, 25505.94641922509, 23960.651414591375, 15104.585127433027, 15135.377576007122, 27369.533945860036, 19367.107009941756, 25184.740199809545, 20926.629633714394, 18223.45090641179, 27335.79338117469, 21664.996329838228, 26506.972832494423, 29395.066282011292, 13081.071401895617, 27020.988508753668, 21106.187637654504, 21873.407920322017, 28171.860911665484, 22464.25062227892, 13564.61117203081, 4026.05720184432, 25767.657332662653, 23721.726758150806, 24144.766846303595, 13249.052950870915, 28778.158946633783, 9981.971001673563, 28159.358241689057, -12296.816146656987, 12818.402300538428, 14857.962071743466, 24029.523354879937, 25437.64192021692, -1207.7742953589725, 19813.146072104515, 24914.09569018615, 17287.290460591765], 'episode_lengths': [151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7583810114396217, 'mean_inference_ms': 1.4136270003586466, 'mean_action_processing_ms': 0.1681794029153136, 'mean_env_wait_ms': 2.4441497025023726, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.021897077560424805, 'StateBufferConnector_ms': 0.00515294075012207, 'ViewRequirementAgentConnector_ms': 0.3126258850097656}, 'num_healthy_workers': 4, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 280000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 280000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 388.47409400646393, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 280000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 280000, 'timers': {'training_iteration_time_ms': 10173.765, 'sample_time_ms': 4787.381, 'synch_weights_time_ms': 7.544}, 'counters': {'num_env_steps_sampled': 280000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 280000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 528, 'training_iteration': 70, 'trial_id': 'default', 'date': '2023-10-08_17-16-19', 'timestamp': 1696785379, 'time_this_iter_s': 10.300770044326782, 'time_total_s': 206.04570746421814, 'pid': 235, 'hostname': 'Muneebs-MacBook-Pro.local', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'apps.environment_simulator.sevice_layer.market_simulator.MarketSimulator'>, 'env_config': {'db_params': {'database': 'traderdb', 'user': 'traderadmin', 'password': 'i4n6s9l1y', 'host': 'localhost', 'port': '5432'}, 'max_episode_steps': 150, 'the_current_time_step': datetime.datetime(1995, 1, 1, 10, 0), 'print_output': False}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x162ae1510>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 4}, 'time_since_restore': 206.04570746421814, 'iterations_since_restore': 20, 'perf': {'cpu_util_percent': 29.593333333333337, 'ram_util_percent': 63.006666666666646}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/simulated_checkpoints/\"\n",
    "\n",
    "algo.save(checkpoint_dir=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 17:18:43,635\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 17:18:43,693\tINFO trainable.py:984 -- Restored on 127.0.0.1 from checkpoint: Checkpoint(filesystem=local, path=/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/simulated_checkpoints/)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.restore(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_current_time_step  = datetime(year=1995, month=1, day=1, hour=10) + relativedelta(hours=0)\n",
    "\n",
    "env_config = {\n",
    "    \"db_params\": db_params, \n",
    "    \"max_episode_steps\": 10, \n",
    "    \"print_output\": True,\n",
    "    \"the_current_time_step\": my_current_time_step,\n",
    "}\n",
    "\n",
    "myEnv = MarketSimulator(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 17:17:25,813\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-10-08 17:17:26,278\tINFO packaging.py:518 -- Creating a file package for local directory '/Users/mirbilal/Desktop/MobCommission/commissionV2/'.\n",
      "2023-10-08 17:17:26,606\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_3b43d8f216855fc5.zip' (10.72MiB) to Ray cluster...\n",
      "2023-10-08 17:17:26,654\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_3b43d8f216855fc5.zip'.\n",
      "2023-10-08 17:17:28,385\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "/Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning:\n",
      "\n",
      "This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=15766)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15766)\u001b[0m /Users/mirbilal/Desktop/MobCommission/commissionV2/v2env/lib/python3.10/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15766)\u001b[0m   logger.deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,455\tWARNING utils.py:152 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,499\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15766)\u001b[0m 2023-10-08 17:17:40,566\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 17:17:40,639\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,574\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,574\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,574\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=15764)\u001b[0m 2023-10-08 17:17:40,575\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 17:17:40,725\tINFO trainable.py:188 -- Trainable.setup took 12.334 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-10-08 17:17:40,751\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 17:17:40,796\tINFO trainable.py:984 -- Restored on 127.0.0.1 from checkpoint: Checkpoint(filesystem=local, path=/Users/mirbilal/Desktop/MobCommission/commissionV2/apps/environment_simulator/simulated_checkpoints/)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(runtime_env={\"working_dir\": \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"})\n",
    "\n",
    "config = {\n",
    "    \"env\": MarketSimulator,\n",
    "    \"env_config\": {\n",
    "        \"db_params\": db_params, \n",
    "        \"max_episode_steps\": 150, \n",
    "        \"the_current_time_step\": my_current_time_step,\n",
    "        \"print_output\": False\n",
    "    },\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "new_algo = ppo.PPO(config=config)\n",
    "new_algo.restore(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stepping_a [899 530 202  37 318]\n",
      "[[1.57822450e+02]\n",
      " [3.10660832e+00]\n",
      " [4.09891726e+02]\n",
      " [8.03168974e+03]\n",
      " [2.80655342e+04]\n",
      " [3.72587265e+03]]\n",
      "[[  87.40602652 1050.52535324  513.30215289   87.4152659   542.74283225\n",
      "    87.42130536   86.8         230.41474654]\n",
      " [  24.88796476 4085.82720641 2159.98151224   24.89193074 2185.64775091\n",
      "    24.89968625   25.          800.        ]\n",
      " [ 162.1910232  3150.27605446 1631.26782715  162.20821719 1425.56075021\n",
      "   162.25684778  158.33333333  126.31578947]\n",
      " [  37.40892211 2451.738319   1217.04005846   37.41010749 1109.92991571\n",
      "    37.41508652   37.5         533.33333333]\n",
      " [  21.68100633 4404.2458886  2227.88930773   21.69219909 2360.39860704\n",
      "    21.71976      21.33333333  937.5       ]]\n",
      "[0]\n",
      "change_in_shares for index: 1 -265 800.0 2159.9815122390487\n",
      "change_in_shares for index: 2 -101 126.3157894736842 1631.267827153499\n",
      "change_in_shares for index: 4 -159 937.5 2227.8893077266875\n",
      "change_in_shares for index: 0 449 230.41474654377882 513.3021528854566\n",
      "crack_2\n",
      "change_in_shares for index: 3 18 533.3333333333334 1217.0400584616184\n",
      "crack_2\n",
      "stepping_b: False 100000.0, 100814.6649615855, 100814.61322441701 -13497.186423509736 -12682.521461924238\n",
      "[[1.57822450e+02]\n",
      " [3.10660832e+00]\n",
      " [4.09891726e+02]\n",
      " [8.03168974e+03]\n",
      " [2.80655342e+04]\n",
      " [3.72587265e+03]]\n",
      "[[  87.40602652 1050.52535324  513.30215289   87.4152659   542.74283225\n",
      "    87.42130536   86.8         532.72607821]\n",
      " [  24.88796476 4085.82720641 2159.98151224   24.89193074 2185.64775091\n",
      "    24.89968625   25.          535.        ]\n",
      " [ 162.1910232  3150.27605446 1631.26782715  162.20821719 1425.56075021\n",
      "   162.25684778  158.33333333   25.31578947]\n",
      " [  37.40892211 2451.738319   1217.04005846   37.41010749 1109.92991571\n",
      "    37.41508652   37.5         533.33333333]\n",
      " [  21.68100633 4404.2458886  2227.88930773   21.69219909 2360.39860704\n",
      "    21.71976      21.33333333  778.5       ]]\n",
      "\n",
      "\n",
      "stepping_a [893 556 637  42  27]\n",
      "[[1.64482560e+02]\n",
      " [2.99690197e+00]\n",
      " [4.13875144e+02]\n",
      " [8.20711009e+03]\n",
      " [2.85707259e+04]\n",
      " [3.81871728e+03]]\n",
      "[[  88.98426233 1061.5364086   500.83095733   88.99765639  503.18043811\n",
      "    89.03309508   87.40602652  532.72607821]\n",
      " [  24.97075999 4101.03375254 2172.86381437   24.97269296 2110.7527598\n",
      "    24.98176157   24.88796476  535.        ]\n",
      " [ 163.33525013 3197.13776955 1678.89604714  163.33917029 1430.73066082\n",
      "   163.35632954  162.1910232    25.31578947]\n",
      " [  37.98739244 2472.64669045 1224.61877096   38.01361581 1127.77656986\n",
      "    38.02299108   37.40892211  533.33333333]\n",
      " [  22.01598173 4424.04964865 2326.45265142   22.03665574 2351.28116309\n",
      "    22.06606957   21.68100633  778.5       ]]\n",
      "[0]\n",
      "change_in_shares for index: 1 -278 535.0 2172.8638143664857\n",
      "change_in_shares for index: 3 -21 533.3333333333334 1224.618770964509\n",
      "change_in_shares for index: 0 446 532.7260782147517 500.8309573296615\n",
      "crack_2\n",
      "change_in_shares for index: 2 318 25.315789473684205 1678.8960471421574\n",
      "crack_2\n",
      "change_in_shares for index: 4 13 778.5 2326.4526514184213\n",
      "crack_2\n",
      "stepping_b: False 100814.61322441701, 102297.93890279185, 102294.78135345568 -84202.23752872886 -82718.91185035402\n",
      "[[1.64482560e+02]\n",
      " [2.99690197e+00]\n",
      " [4.13875144e+02]\n",
      " [8.20711009e+03]\n",
      " [2.85707259e+04]\n",
      " [3.81871728e+03]]\n",
      "[[  88.98426233 1061.5364086   500.83095733   88.99765639  503.18043811\n",
      "    89.03309508   87.40602652  619.66784484]\n",
      " [  24.97075999 4101.03375254 2172.86381437   24.97269296 2110.7527598\n",
      "    24.98176157   24.88796476  257.        ]\n",
      " [ 163.33525013 3197.13776955 1678.89604714  163.33917029 1430.73066082\n",
      "   163.35632954  162.1910232    25.31578947]\n",
      " [  37.98739244 2472.64669045 1224.61877096   38.01361581 1127.77656986\n",
      "    38.02299108   37.40892211  512.33333333]\n",
      " [  22.01598173 4424.04964865 2326.45265142   22.03665574 2351.28116309\n",
      "    22.06606957   21.68100633  778.5       ]]\n",
      "\n",
      "\n",
      "stepping_a [729  65 940 810 675]\n",
      "[[1.69768307e+02]\n",
      " [2.87868951e+00]\n",
      " [4.17447842e+02]\n",
      " [8.34689651e+03]\n",
      " [2.90847228e+04]\n",
      " [3.90846597e+03]]\n",
      "[[  89.94958923 1062.66287313  487.85202249   89.95753259  513.03472978\n",
      "    89.98850002   88.98426233  619.66784484]\n",
      " [  24.9959054  4105.7475498  2230.12955526   24.99753446 2040.07430021\n",
      "    24.9989203    24.97075999  257.        ]\n",
      " [ 166.87196142 3202.60767051 1728.24572151  166.88947976 1467.654928\n",
      "   166.94080258  163.33525013   25.31578947]\n",
      " [  38.00605849 2494.26468743 1271.84274385   38.00617737 1148.07785412\n",
      "    38.00745437   37.98739244  512.33333333]\n",
      " [  22.05603504 4449.02276824 2307.95110805   22.05694114 2313.97075951\n",
      "    22.0624057    22.01598173  778.5       ]]\n",
      "[0]\n",
      "change_in_shares for index: 2 -470 25.315789473684205 1728.2457215093825\n",
      "break_3\n",
      "change_in_shares for index: 3 -405 512.3333333333334 1271.8427438476517\n",
      "change_in_shares for index: 0 364 619.6678448363182 487.8520224920045\n",
      "crack_2\n",
      "change_in_shares for index: 1 32 257.0 2230.1295552627553\n",
      "crack_2\n",
      "change_in_shares for index: 4 337 778.5 2307.9511080548473\n",
      "crack_2\n",
      "stepping_b: True 102294.78135345568, 103029.7051509085, 103021.76890374525 -21984.94309623079 -21250.019298777966\n",
      "[[1.69768307e+02]\n",
      " [2.87868951e+00]\n",
      " [4.17447842e+02]\n",
      " [8.34689651e+03]\n",
      " [2.90847228e+04]\n",
      " [3.90846597e+03]]\n",
      "[[8.99495892e+01 1.06266287e+03 4.87852022e+02 8.99575326e+01\n",
      "  5.13034730e+02 8.99885000e+01 8.89842623e+01 8.35812700e+02]\n",
      " [2.49959054e+01 4.10574755e+03 2.23012956e+03 2.49975345e+01\n",
      "  2.04007430e+03 2.49989203e+01 2.49707600e+01 2.57000000e+02]\n",
      " [1.66871961e+02 3.20260767e+03 1.72824572e+03 1.66889480e+02\n",
      "  1.46765493e+03 1.66940803e+02 1.63335250e+02 1.00000000e+00]\n",
      " [3.80060585e+01 2.49426469e+03 1.27184274e+03 3.80061774e+01\n",
      "  1.14807785e+03 3.80074544e+01 3.79873924e+01 1.07333333e+02]\n",
      " [2.20560350e+01 4.44902277e+03 2.30795111e+03 2.20569411e+01\n",
      "  2.31397076e+03 2.20624057e+01 2.20159817e+01 7.78500000e+02]]\n",
      "\n",
      "\n",
      "stepping_a [360 694 718 166 395]\n",
      "[[1.72595149e+02]\n",
      " [2.82156294e+00]\n",
      " [4.20922668e+02]\n",
      " [8.47485288e+03]\n",
      " [2.89524317e+04]\n",
      " [3.96095080e+03]]\n",
      "[[8.85255746e+01 1.06752386e+03 4.85519870e+02 8.85371184e+01\n",
      "  5.08374131e+02 8.85858216e+01 8.99495892e+01 8.35812700e+02]\n",
      " [2.51395868e+01 4.10991249e+03 2.12917630e+03 2.51439945e+01\n",
      "  2.01965624e+03 2.51606292e+01 2.49959054e+01 2.57000000e+02]\n",
      " [1.72065673e+02 3.20394527e+03 1.73444764e+03 1.72097666e+02\n",
      "  1.46694476e+03 1.72162613e+02 1.66871961e+02 1.00000000e+00]\n",
      " [3.85151850e+01 2.49953264e+03 1.32704937e+03 3.85280397e+01\n",
      "  1.18691123e+03 3.85564341e+01 3.80060585e+01 1.07333333e+02]\n",
      " [2.19003453e+01 4.45445259e+03 2.33232812e+03 2.19121326e+01\n",
      "  2.24623108e+03 2.19189132e+01 2.20560350e+01 7.78500000e+02]]\n",
      "[0]\n",
      "change_in_shares for index: 0 -180 835.8126998634445 485.51986976275606\n",
      "change_in_shares for index: 1 -347 257.0 2129.1763022639375\n",
      "break_3\n",
      "change_in_shares for index: 2 -359 1.0 1734.4476408822818\n",
      "break_3\n",
      "change_in_shares for index: 3 -83 107.33333333333337 1327.0493748289884\n",
      "change_in_shares for index: 4 197 778.5 2332.3281226355543\n",
      "stepping_b: True 103021.76890374525, 101807.12100910339, 101807.73634091206 -448.0 -1662.6478946418647\n",
      "[[1.72595149e+02]\n",
      " [2.82156294e+00]\n",
      " [4.20922668e+02]\n",
      " [8.47485288e+03]\n",
      " [2.89524317e+04]\n",
      " [3.96095080e+03]]\n",
      "[[8.85255746e+01 1.06752386e+03 4.85519870e+02 8.85371184e+01\n",
      "  5.08374131e+02 8.85858216e+01 8.99495892e+01 6.55812700e+02]\n",
      " [2.51395868e+01 4.10991249e+03 2.12917630e+03 2.51439945e+01\n",
      "  2.01965624e+03 2.51606292e+01 2.49959054e+01 1.00000000e+00]\n",
      " [1.72065673e+02 3.20394527e+03 1.73444764e+03 1.72097666e+02\n",
      "  1.46694476e+03 1.72162613e+02 1.66871961e+02 1.00000000e+00]\n",
      " [3.85151850e+01 2.49953264e+03 1.32704937e+03 3.85280397e+01\n",
      "  1.18691123e+03 3.85564341e+01 3.80060585e+01 2.43333333e+01]\n",
      " [2.19003453e+01 4.45445259e+03 2.33232812e+03 2.19121326e+01\n",
      "  2.24623108e+03 2.19189132e+01 2.20560350e+01 9.75500000e+02]]\n",
      "\n",
      "\n",
      "stepping_a [627 238 324 930 824]\n",
      "[[1.76549093e+02]\n",
      " [2.77114604e+00]\n",
      " [4.25912491e+02]\n",
      " [8.63161164e+03]\n",
      " [2.88244053e+04]\n",
      " [4.01311082e+03]]\n",
      "[[8.89331220e+01 1.08138926e+03 4.77580417e+02 8.89393115e+01\n",
      "  4.87246324e+02 8.89476012e+01 8.85255746e+01 6.55812700e+02]\n",
      " [2.52079614e+01 4.11379498e+03 2.09891915e+03 2.52100155e+01\n",
      "  2.09857539e+03 2.52162386e+01 2.51395868e+01 1.00000000e+00]\n",
      " [1.75171675e+02 3.24387626e+03 1.74173920e+03 1.75188320e+02\n",
      "  1.47539436e+03 1.75228193e+02 1.72065673e+02 1.00000000e+00]\n",
      " [3.95087918e+01 2.52075197e+03 1.35064378e+03 3.95317324e+01\n",
      "  1.17884213e+03 3.95875708e+01 3.85151850e+01 2.43333333e+01]\n",
      " [2.20962328e+01 4.46289628e+03 2.27320987e+03 2.21015689e+01\n",
      "  2.24743259e+03 2.21198656e+01 2.19003453e+01 9.75500000e+02]]\n",
      "[21253]\n",
      "change_in_shares for index: 1 -119 1.0 2098.9191491990405\n",
      "break_3\n",
      "change_in_shares for index: 2 -162 1.0 1741.739197552333\n",
      "break_3\n",
      "change_in_shares for index: 3 -465 24.33333333333337 1350.6437807996624\n",
      "break_3\n",
      "change_in_shares for index: 4 -412 975.5 2273.2098723264553\n",
      "change_in_shares for index: 0 313 655.8126998634445 477.5804171242437\n",
      "stepping_b: True 101807.39103608503, 102293.10622751634, 102291.30801046119 -719.6666666666666 -233.95147523535218\n",
      "[[1.76549093e+02]\n",
      " [2.77114604e+00]\n",
      " [4.25912491e+02]\n",
      " [8.63161164e+03]\n",
      " [2.88244053e+04]\n",
      " [4.01311082e+03]]\n",
      "[[8.89331220e+01 1.08138926e+03 4.77580417e+02 8.89393115e+01\n",
      "  4.87246324e+02 8.89476012e+01 8.85255746e+01 9.68812700e+02]\n",
      " [2.52079614e+01 4.11379498e+03 2.09891915e+03 2.52100155e+01\n",
      "  2.09857539e+03 2.52162386e+01 2.51395868e+01 1.00000000e+00]\n",
      " [1.75171675e+02 3.24387626e+03 1.74173920e+03 1.75188320e+02\n",
      "  1.47539436e+03 1.75228193e+02 1.72065673e+02 1.00000000e+00]\n",
      " [3.95087918e+01 2.52075197e+03 1.35064378e+03 3.95317324e+01\n",
      "  1.17884213e+03 3.95875708e+01 3.85151850e+01 1.00000000e+00]\n",
      " [2.20962328e+01 4.46289628e+03 2.27320987e+03 2.21015689e+01\n",
      "  2.24743259e+03 2.21198656e+01 2.19003453e+01 5.63500000e+02]]\n",
      "\n",
      "\n",
      "stepping_a [646 335 208 900 801]\n",
      "[[1.80335976e+02]\n",
      " [2.73867355e+00]\n",
      " [4.32491969e+02]\n",
      " [8.76864096e+03]\n",
      " [2.86513859e+04]\n",
      " [4.07877739e+03]]\n",
      "[[8.78309661e+01 1.08328868e+03 4.67404773e+02 8.78426367e+01\n",
      "  4.87375418e+02 8.78821560e+01 8.89331220e+01 9.68812700e+02]\n",
      " [2.53347295e+01 4.11464228e+03 2.13400466e+03 2.53396087e+01\n",
      "  2.11463018e+03 2.53540005e+01 2.52079614e+01 1.00000000e+00]\n",
      " [1.78815203e+02 3.26095240e+03 1.74752331e+03 1.78820789e+02\n",
      "  1.44757783e+03 1.78860923e+02 1.75171675e+02 1.00000000e+00]\n",
      " [4.06519254e+01 2.52567577e+03 1.40974606e+03 4.06656233e+01\n",
      "  1.14201083e+03 4.07325406e+01 3.95087918e+01 1.00000000e+00]\n",
      " [2.22780128e+01 4.54563876e+03 2.19052236e+03 2.22907616e+01\n",
      "  2.28355142e+03 2.23088348e+01 2.20962328e+01 5.63500000e+02]]\n",
      "[3440]\n",
      "change_in_shares for index: 0 -323 968.8126998634445 467.40477303656064\n",
      "change_in_shares for index: 2 -104 1.0 1747.5233071022708\n",
      "break_3\n",
      "change_in_shares for index: 3 -450 1.0 1409.746058815464\n",
      "break_3\n",
      "change_in_shares for index: 1 167 1.0 2134.0046645776115\n",
      "change_in_shares for index: 4 400 563.5 2190.5223561057987\n",
      "stepping_b: True 102290.65367652303, 101330.21748147007, 101318.44000523916 -552.0 -1512.4361950529565\n",
      "[[1.80335976e+02]\n",
      " [2.73867355e+00]\n",
      " [4.32491969e+02]\n",
      " [8.76864096e+03]\n",
      " [2.86513859e+04]\n",
      " [4.07877739e+03]]\n",
      "[[8.78309661e+01 1.08328868e+03 4.67404773e+02 8.78426367e+01\n",
      "  4.87375418e+02 8.78821560e+01 8.89331220e+01 6.45812700e+02]\n",
      " [2.53347295e+01 4.11464228e+03 2.13400466e+03 2.53396087e+01\n",
      "  2.11463018e+03 2.53540005e+01 2.52079614e+01 1.68000000e+02]\n",
      " [1.78815203e+02 3.26095240e+03 1.74752331e+03 1.78820789e+02\n",
      "  1.44757783e+03 1.78860923e+02 1.75171675e+02 1.00000000e+00]\n",
      " [4.06519254e+01 2.52567577e+03 1.40974606e+03 4.06656233e+01\n",
      "  1.14201083e+03 4.07325406e+01 3.95087918e+01 1.00000000e+00]\n",
      " [2.22780128e+01 4.54563876e+03 2.19052236e+03 2.22907616e+01\n",
      "  2.28355142e+03 2.23088348e+01 2.20962328e+01 9.63500000e+02]]\n",
      "\n",
      "\n",
      "stepping_a [219  61 305 752 868]\n",
      "[[1.84644276e+02]\n",
      " [2.63650920e+00]\n",
      " [4.39170154e+02]\n",
      " [8.97930216e+03]\n",
      " [2.86350273e+04]\n",
      " [4.13207850e+03]]\n",
      "[[8.71087426e+01 1.10824115e+03 4.70436142e+02 8.71150286e+01\n",
      "  4.79553851e+02 8.71306706e+01 8.78309661e+01 6.45812700e+02]\n",
      " [2.54877524e+01 4.12739787e+03 2.14812601e+03 2.54938967e+01\n",
      "  2.16070198e+03 2.55056380e+01 2.53347295e+01 1.68000000e+02]\n",
      " [1.85414294e+02 3.31314616e+03 1.81672699e+03 1.85440009e+02\n",
      "  1.44128160e+03 1.85525312e+02 1.78815203e+02 1.00000000e+00]\n",
      " [4.13731009e+01 2.54584338e+03 1.41176364e+03 4.13945964e+01\n",
      "  1.15847855e+03 4.14419822e+01 4.06519254e+01 1.00000000e+00]\n",
      " [2.21995369e+01 4.54794912e+03 2.26996751e+03 2.22032517e+01\n",
      "  2.23475620e+03 2.22114015e+01 2.22780128e+01 9.63500000e+02]]\n",
      "[18655]\n",
      "change_in_shares for index: 3 -376 1.0 1411.7636378149252\n",
      "break_3\n",
      "change_in_shares for index: 4 -434 963.5 2269.96750904791\n",
      "change_in_shares for index: 0 109 645.8126998634445 470.43614211097963\n",
      "change_in_shares for index: 1 30 168.0 2148.12600712022\n",
      "change_in_shares for index: 2 152 1.0 1816.7269867801222\n",
      "crack_2\n",
      "stepping_b: True 101317.9203743313, 100808.91588679852, 100796.81293117211 -10546.04842946434 -11055.052916997112\n",
      "[[1.84644276e+02]\n",
      " [2.63650920e+00]\n",
      " [4.39170154e+02]\n",
      " [8.97930216e+03]\n",
      " [2.86350273e+04]\n",
      " [4.13207850e+03]]\n",
      "[[8.71087426e+01 1.10824115e+03 4.70436142e+02 8.71150286e+01\n",
      "  4.79553851e+02 8.71306706e+01 8.78309661e+01 7.54812700e+02]\n",
      " [2.54877524e+01 4.12739787e+03 2.14812601e+03 2.54938967e+01\n",
      "  2.16070198e+03 2.55056380e+01 2.53347295e+01 1.98000000e+02]\n",
      " [1.85414294e+02 3.31314616e+03 1.81672699e+03 1.85440009e+02\n",
      "  1.44128160e+03 1.85525312e+02 1.78815203e+02 9.81770310e+01]\n",
      " [4.13731009e+01 2.54584338e+03 1.41176364e+03 4.13945964e+01\n",
      "  1.15847855e+03 4.14419822e+01 4.06519254e+01 1.00000000e+00]\n",
      " [2.21995369e+01 4.54794912e+03 2.26996751e+03 2.22032517e+01\n",
      "  2.23475620e+03 2.22114015e+01 2.22780128e+01 5.29500000e+02]]\n",
      "\n",
      "\n",
      "stepping_a [376 563 141 961 394]\n",
      "[[1.87208231e+02]\n",
      " [2.57277234e+00]\n",
      " [4.37336536e+02]\n",
      " [9.10381225e+03]\n",
      " [2.85750572e+04]\n",
      " [4.16964452e+03]]\n",
      "[[8.78777945e+01 1.10883613e+03 4.87932804e+02 8.78909053e+01\n",
      "  4.78936521e+02 8.79028733e+01 8.71087426e+01 7.54812700e+02]\n",
      " [2.54988811e+01 4.14057606e+03 2.17952289e+03 2.54991358e+01\n",
      "  2.19448978e+03 2.55001626e+01 2.54877524e+01 1.98000000e+02]\n",
      " [1.89120792e+02 3.34730433e+03 1.80372338e+03 1.89140760e+02\n",
      "  1.44877110e+03 1.89172992e+02 1.85414294e+02 9.81770310e+01]\n",
      " [4.14240123e+01 2.55195996e+03 1.45321387e+03 4.14253956e+01\n",
      "  1.14707637e+03 4.14275648e+01 4.13731009e+01 1.00000000e+00]\n",
      " [2.23952050e+01 4.55834292e+03 2.36182778e+03 2.24010080e+01\n",
      "  2.20728996e+03 2.24278283e+01 2.21995369e+01 5.29500000e+02]]\n",
      "[0]\n",
      "change_in_shares for index: 0 -188 754.8126998634445 487.93280367742454\n",
      "change_in_shares for index: 4 -197 529.5 2361.8277788644114\n",
      "change_in_shares for index: 1 281 198.0 2179.5228926708564\n",
      "change_in_shares for index: 2 70 98.17703102559437 1803.7233813946732\n",
      "change_in_shares for index: 3 480 1.0 1453.2138730728755\n",
      "crack_2\n",
      "stepping_b: False 100796.81293117211, 101847.05670821245, 101846.6052360754 -19356.39748854422 -18306.153711503885\n",
      "[[1.87208231e+02]\n",
      " [2.57277234e+00]\n",
      " [4.37336536e+02]\n",
      " [9.10381225e+03]\n",
      " [2.85750572e+04]\n",
      " [4.16964452e+03]]\n",
      "[[  87.87779453 1108.83613264  487.93280368   87.89090533  478.93652066\n",
      "    87.90287327   87.10874262  566.81269986]\n",
      " [  25.49888114 4140.57606448 2179.52289267   25.49913581 2194.48978117\n",
      "    25.50016261   25.48775236  479.        ]\n",
      " [ 189.12079164 3347.30433008 1803.72338139  189.1407604  1448.77110156\n",
      "   189.17299228  185.4142936   168.17703103]\n",
      " [  41.4240123  2551.95995824 1453.21387307   41.42539564 1147.07636637\n",
      "    41.42756481   41.3731009    13.76525958]\n",
      " [  22.39520501 4558.34291788 2361.82777886   22.40100797 2207.2899644\n",
      "    22.42782826   22.19953694  332.5       ]]\n",
      "\n",
      "\n",
      "stepping_a [827 554 132 779 735]\n",
      "[[1.88990726e+02]\n",
      " [2.51323649e+00]\n",
      " [4.37354377e+02]\n",
      " [9.22176635e+03]\n",
      " [2.84648748e+04]\n",
      " [4.20845447e+03]]\n",
      "[[  88.62422154 1111.82782696  497.43403741   88.62897597  480.00279345\n",
      "    88.6550742    87.87779453  566.81269986]\n",
      " [  25.56262029 4144.79350957 2223.9636078    25.56543942 2207.45952277\n",
      "    25.57244506   25.49888114  479.        ]\n",
      " [ 192.31318854 3370.34857626 1851.94167473  192.34013091 1478.70763285\n",
      "   192.36994701  189.12079164  168.17703103]\n",
      " [  41.32503923 2559.5100881  1467.03157767   41.32748066 1134.06368722\n",
      "    41.3317475    41.4240123    13.76525958]\n",
      " [  22.54274955 4570.04374886 2321.1098882    22.54719027 2177.40857145\n",
      "    22.56133676   22.39520501  332.5       ]]\n",
      "[0]\n",
      "change_in_shares for index: 1 -277 479.0 2223.963607795568\n",
      "change_in_shares for index: 2 -66 168.17703102559437 1851.9416747343669\n",
      "change_in_shares for index: 0 413 566.8126998634445 497.4340374100561\n",
      "crack_2\n",
      "change_in_shares for index: 3 389 13.765259582659848 1467.0315776651541\n",
      "crack_2\n",
      "change_in_shares for index: 4 367 332.5 2321.1098882041115\n",
      "crack_2\n",
      "stepping_b: False 101846.6052360754, 102884.80459728504, 102880.48146457468 -41196.53065533191 -40158.33129412227\n",
      "[[1.88990726e+02]\n",
      " [2.51323649e+00]\n",
      " [4.37354377e+02]\n",
      " [9.22176635e+03]\n",
      " [2.84648748e+04]\n",
      " [4.20845447e+03]]\n",
      "[[  88.62422154 1111.82782696  497.43403741   88.62897597  480.00279345\n",
      "    88.6550742    87.87779453  789.8803081 ]\n",
      " [  25.56262029 4144.79350957 2223.9636078    25.56543942 2207.45952277\n",
      "    25.57244506   25.49888114  202.        ]\n",
      " [ 192.31318854 3370.34857626 1851.94167473  192.34013091 1478.70763285\n",
      "   192.36994701  189.12079164  102.17703103]\n",
      " [  41.32503923 2559.5100881  1467.03157767   41.32748066 1134.06368722\n",
      "    41.3317475    41.4240123    13.76525958]\n",
      " [  22.54274955 4570.04374886 2321.1098882    22.54719027 2177.40857145\n",
      "    22.56133676   22.39520501  332.5       ]]\n",
      "\n",
      "\n",
      "stepping_a [262 425  63 227 696]\n",
      "[[1.93381720e+02]\n",
      " [2.44646204e+00]\n",
      " [4.37127135e+02]\n",
      " [9.33659470e+03]\n",
      " [2.88807376e+04]\n",
      " [4.26848938e+03]]\n",
      "[[  91.82556296 1152.23814094  503.85230523   91.87373523  509.06485002\n",
      "    91.95371664   88.62422154  789.8803081 ]\n",
      " [  25.54835227 4160.2519795  2201.30277087   25.54889286 2130.85803247\n",
      "    25.55021189   25.56262029  202.        ]\n",
      " [ 194.01632128 3410.05876808 1867.43189893  194.02897634 1482.40053227\n",
      "   194.04328501  192.31318854  102.17703103]\n",
      " [  41.11687242 2567.23426528 1464.88123986   41.12126852 1136.73837756\n",
      "    41.1325553    41.32503923   13.76525958]\n",
      " [  22.70868786 4594.37271145 2394.80908945   22.71269102 2208.50561496\n",
      "    22.72227715   22.54274955  332.5       ]]\n",
      "[0]\n",
      "change_in_shares for index: 0 -131 789.8803080992236 503.8523052263194\n",
      "change_in_shares for index: 4 -348 332.5 2394.8090894457973\n",
      "break_3\n",
      "change_in_shares for index: 1 212 202.0 2201.3027708691684\n",
      "change_in_shares for index: 2 31 102.17703102559437 1867.4318989284398\n",
      "change_in_shares for index: 3 113 13.765259582659848 1464.8812398588657\n",
      "stepping_b: True 102880.48146457468, 105632.60593350408, 105637.2412685611 -15.5 2736.6244689293962\n",
      "[[1.93381720e+02]\n",
      " [2.44646204e+00]\n",
      " [4.37127135e+02]\n",
      " [9.33659470e+03]\n",
      " [2.88807376e+04]\n",
      " [4.26848938e+03]]\n",
      "[[9.18255630e+01 1.15223814e+03 5.03852305e+02 9.18737352e+01\n",
      "  5.09064850e+02 9.19537166e+01 8.86242215e+01 6.58880308e+02]\n",
      " [2.55483523e+01 4.16025198e+03 2.20130277e+03 2.55488929e+01\n",
      "  2.13085803e+03 2.55502119e+01 2.55626203e+01 4.14000000e+02]\n",
      " [1.94016321e+02 3.41005877e+03 1.86743190e+03 1.94028976e+02\n",
      "  1.48240053e+03 1.94043285e+02 1.92313189e+02 1.33177031e+02]\n",
      " [4.11168724e+01 2.56723427e+03 1.46488124e+03 4.11212685e+01\n",
      "  1.13673838e+03 4.11325553e+01 4.13250392e+01 1.26765260e+02]\n",
      " [2.27086879e+01 4.59437271e+03 2.39480909e+03 2.27126910e+01\n",
      "  2.20850561e+03 2.27222771e+01 2.25427495e+01 1.00000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "def get_latest_states():\n",
    "    latest_states = []\n",
    "    env = myEnv\n",
    "    env.reset()\n",
    "    obs = env.state\n",
    "    latest_states = []  # To store the latest states from each worker\n",
    "    for _ in range(10):  # Interact with the environment for 10 steps as an example\n",
    "        action = algo.compute_single_action(obs)  # Replace with your agent's action\n",
    "        # action = np.array([[664, 831,  51, 988, 280,],\n",
    "        #             [  1,   0,   1,   1,   0]])\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        state = env.state  # Obtain the state from the environment\n",
    "        latest_states.append(copy.deepcopy(obs))\n",
    "    return latest_states\n",
    "\n",
    "latest_states = get_latest_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2368.5787505716144\n",
      "\n",
      "-1841.1675964013139\n",
      "\n",
      "-1277.4526141225815\n",
      "\n",
      "-1976.5484557908585\n",
      "\n",
      "-1140.0827290324942\n",
      "\n",
      "-1007.7464672388451\n",
      "\n",
      "-1642.0243037815926\n",
      "\n",
      "-1403.3877523770698\n",
      "\n",
      "-1057.3095151802063\n",
      "\n",
      "-1183.6614307732852\n",
      "\n",
      "total_rewards -14897.959615269861\n"
     ]
    }
   ],
   "source": [
    "total_portfolios = []\n",
    "total_rewards = 0\n",
    "for astate in latest_states:\n",
    "    # print(astate)\n",
    "    total_portfolio_a = 0\n",
    "    total_portfolio_b = 0\n",
    "    (stock_state, commodity_state, wallet_state) = astate\n",
    "    total_portfolio_b = total_portfolio_b + wallet_state[0]\n",
    "    total_portfolio_a = total_portfolio_a + wallet_state[0]\n",
    "    # print(stock_state)\n",
    "    gain = 0\n",
    "    for a_stck in stock_state:\n",
    "        total_portfolio_a = total_portfolio_a + a_stck[6]*a_stck[7]\n",
    "        total_portfolio_b = total_portfolio_b + a_stck[0]*a_stck[7]\n",
    "        gain = gain + (a_stck[0]*a_stck[7] - a_stck[6]*a_stck[7])\n",
    "    print(gain)\n",
    "    print(\"\")\n",
    "    total_rewards = total_rewards + gain\n",
    "        \n",
    "    # print((total_portfolio_a, total_portfolio_b))\n",
    "    total_portfolios.append((total_portfolio_a, total_portfolio_b))\n",
    "\n",
    "print(\"total_rewards\", total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor (Quotient): 10\n",
      "Remainder: 1\n"
     ]
    }
   ],
   "source": [
    "dividend = 21\n",
    "divisor = 2\n",
    "\n",
    "factor = dividend // divisor  # Integer division\n",
    "remainder = dividend % divisor  # Modulo operation\n",
    "\n",
    "print(f\"Factor (Quotient): {factor}\")\n",
    "print(f\"Remainder: {remainder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2env",
   "language": "python",
   "name": "v2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
