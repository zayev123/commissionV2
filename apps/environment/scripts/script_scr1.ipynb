{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import django\n",
    "import os\n",
    "file_dir = \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"\n",
    "if file_dir not in sys.path:\n",
    "    sys.path.insert(0, file_dir)\n",
    "\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"commissionerv2.settings\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\" \n",
    "django.setup()\n",
    "from datetime import datetime, date\n",
    "from psx import stocks, tickers\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import investpy\n",
    "import yfinance as yf\n",
    "from matplotlib.dates import relativedelta\n",
    "from apps.environment.models import Stock\n",
    "from apps.environment.models.stock import StockBuffer\n",
    "from apps.environment.models.commodity import Commodity\n",
    "from apps.environment.models.commodity import CommodityBuffer\n",
    "import pytz\n",
    "from os import environ\n",
    "from math import ceil, floor\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tickers is your DataFrame\n",
    "\n",
    "\n",
    "tickers_column = tickers()\n",
    "# display(tickers_column)\n",
    "symbols = list(tickers_column[\"symbol\"])\n",
    "names = list(tickers_column[\"name\"])\n",
    "len_syms = len(symbols)\n",
    "# # print(symbols)\n",
    "hundred_inds = pd.read_csv('KSE100 indx Comps - stock-exchange-kse-100pakistan.csv').to_dict(orient='records')\n",
    "# print(hundred_inds)\n",
    "hun_syms = {}\n",
    "\n",
    "all_syms = {}\n",
    "for ind in range(len_syms):\n",
    "    all_syms[symbols[ind]] = names[ind]\n",
    "\n",
    "the_index = 1\n",
    "for hun_ind in hundred_inds:\n",
    "    if hun_ind[\"SYMBOL\"] in all_syms:\n",
    "        sym_b = hun_ind[\"SYMBOL\"]\n",
    "        if sym_b not in hun_syms:\n",
    "            hun_syms[sym_b] = {\n",
    "                \"index\": the_index,\n",
    "                \"name\": all_syms[sym_b]\n",
    "            }\n",
    "            the_index = the_index + 1\n",
    "\n",
    "db_stcks = {}\n",
    "all_stcks = Stock.objects.all()\n",
    "max_indx = len(all_stcks)\n",
    "for astck in all_stcks:\n",
    "    db_stcks[astck.symbol] = astck.symbol\n",
    "\n",
    "new_stcks_list = []\n",
    "for sym_bl in hun_syms:\n",
    "    if sym_bl not in db_stcks:\n",
    "        sym_data = hun_syms[sym_bl]\n",
    "        new_indx = sym_data[\"index\"]\n",
    "        if new_indx<=max_indx:\n",
    "            new_indx = max_indx+1\n",
    "        max_indx = max_indx+1\n",
    "        new_stcks_list.append(\n",
    "            Stock(\n",
    "                index=sym_data[\"index\"],\n",
    "                symbol=sym_bl,\n",
    "                name=sym_data[\"name\"],\n",
    "                type=100\n",
    "            )\n",
    "        )\n",
    "\n",
    "Stock.objects.bulk_create(new_stcks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_stocks = Stock.objects.all()\n",
    "\n",
    "stock_syms = {}\n",
    "\n",
    "for a_stock in all_stocks:\n",
    "    if a_stock.symbol not in stock_syms:\n",
    "        stock_syms[a_stock.symbol] = {\n",
    "            \"object\": a_stock,\n",
    "            \"snapshots\": {},\n",
    "            \"last_date\": None\n",
    "        }\n",
    "\n",
    "snapshots = StockBuffer.objects.select_related(\"stock\").all()\n",
    "for snapshot in snapshots:\n",
    "    snp_stck = snapshot.stock\n",
    "    snp_stck_symbl = snp_stck.symbol\n",
    "    if snp_stck_symbl in stock_syms:\n",
    "        snp_data = stock_syms[snp_stck_symbl]\n",
    "        stck_snps = snp_data[\"snapshots\"]\n",
    "        if snapshot.captured_at not in stck_snps:\n",
    "            stck_snps[snapshot.captured_at.strftime('%Y-%m-%d %H:%M:%S')] = snapshot\n",
    "            if not snp_data[\"last_date\"]:\n",
    "                snp_data[\"last_date\"] = snapshot.captured_at.date()\n",
    "\n",
    "# tst = 1\n",
    "for stck_sym, stck_data in stock_syms.items():\n",
    "    new_snapshots = []\n",
    "    last_date: datetime = stck_data[\"last_date\"]\n",
    "    if last_date is None:\n",
    "        start_from = date(2020, 1, 1)\n",
    "    else:\n",
    "        start_from = last_date\n",
    "    print(last_date, stck_sym, start_from)\n",
    "    end_date = pytz.utc.localize(datetime.now() + relativedelta(days=2)).date()\n",
    "    data = stocks(stck_sym, start=start_from, end=end_date)\n",
    "    data_points_len = len(data)\n",
    "    stck_snpshts = stck_data[\"snapshots\"]\n",
    "    the_stock: Stock = stck_data[\"object\"]\n",
    "    last_data_point = None\n",
    "    for digi in range(data_points_len):\n",
    "        data_point = data.iloc[digi]\n",
    "        time_point = data_point.name.to_pydatetime()\n",
    "        time_point_str = data_point.name.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if time_point_str not in stck_snpshts:\n",
    "            zone_point_time = pytz.utc.localize(time_point)\n",
    "            nw_price = data_point.Close\n",
    "            if last_data_point is None:\n",
    "                change = 0\n",
    "            else:\n",
    "                old_price = last_data_point.Close\n",
    "                if old_price and old_price > 0:\n",
    "                    change = (nw_price-old_price)/old_price\n",
    "                else:\n",
    "                    change = 0\n",
    "            new_snapshots.append(StockBuffer(\n",
    "                stock = the_stock,\n",
    "                captured_at = zone_point_time,\n",
    "                price_snapshot = nw_price,\n",
    "                change = change,\n",
    "                volume = data_point.Volume,\n",
    "                bid_vol = 100000000,\n",
    "                bid_price = nw_price,\n",
    "                offer_vol = 100000000,\n",
    "                offer_price = nw_price,\n",
    "                open = data_point.Open,\n",
    "                close = nw_price,\n",
    "                high = data_point.High,\n",
    "                low = data_point.Low\n",
    "            ))\n",
    "        last_data_point = data_point\n",
    "    StockBuffer.objects.bulk_create(new_snapshots)\n",
    "\n",
    "    # tst = tst + 1\n",
    "    # if tst == 4:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_commodities = Commodity.objects.all()\n",
    "\n",
    "commodity_syms = {}\n",
    "\n",
    "for a_commodity in all_commodities:\n",
    "    if a_commodity.symbol not in commodity_syms:\n",
    "        commodity_syms[a_commodity.symbol] = {\n",
    "            \"object\": a_commodity,\n",
    "            \"snapshots\": {},\n",
    "            \"last_date\": None\n",
    "        }\n",
    "\n",
    "snapshots = CommodityBuffer.objects.select_related(\"commodity\").all()\n",
    "for snapshot in snapshots:\n",
    "    snp_cmmdty = snapshot.commodity\n",
    "    snp_cmmdty_symbl = snp_cmmdty.symbol\n",
    "    if snp_cmmdty_symbl in commodity_syms:\n",
    "        cmmdty_data = commodity_syms[snp_cmmdty_symbl]\n",
    "        cmmdty_snps = cmmdty_data[\"snapshots\"]\n",
    "        if snapshot.captured_at not in cmmdty_snps:\n",
    "            cmmdty_snps[snapshot.captured_at.strftime('%Y-%m-%d %H:%M:%S')] = snapshot\n",
    "            if not cmmdty_data[\"last_date\"]:\n",
    "                cmmdty_data[\"last_date\"] = snapshot.captured_at.date()\n",
    "\n",
    "# tst = 1\n",
    "for cmmdty_sym, cmmdty_data in commodity_syms.items():\n",
    "    new_snapshots = []\n",
    "    last_date: datetime = cmmdty_data[\"last_date\"]\n",
    "    if last_date is None:\n",
    "        start_from = date(2019, 12, 30)\n",
    "    else:\n",
    "        start_from = date(2019, 12, 30)\n",
    "        # start_from = last_date\n",
    "    print(last_date, cmmdty_sym, start_from)\n",
    "    end_date = pytz.utc.localize(datetime.now() + relativedelta(days=2)).date()\n",
    "    data = yf.download(cmmdty_sym, start=start_from, end=end_date)\n",
    "    data_points_len = len(data)\n",
    "    cmmdty_snpshts = cmmdty_data[\"snapshots\"]\n",
    "    the_commodity: Commodity = cmmdty_data[\"object\"]\n",
    "    last_data_point = None\n",
    "    for digi in range(data_points_len):\n",
    "        data_point = data.iloc[digi]\n",
    "        time_point = data_point.name.to_pydatetime()\n",
    "        time_point_str = data_point.name.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if time_point_str not in cmmdty_snpshts:\n",
    "            zone_point_time = pytz.utc.localize(time_point)\n",
    "            nw_price = data_point.Close\n",
    "            if last_data_point is None:\n",
    "                change = 0\n",
    "            else:\n",
    "                old_price = last_data_point.Close\n",
    "                if old_price and old_price > 0:\n",
    "                    change = (nw_price-old_price)/old_price\n",
    "                else:\n",
    "                    change = 0\n",
    "            new_snapshots.append(CommodityBuffer(\n",
    "                commodity = the_commodity,\n",
    "                captured_at = zone_point_time,\n",
    "                price_snapshot = nw_price,\n",
    "            ))\n",
    "        last_data_point = data_point\n",
    "    CommodityBuffer.objects.bulk_create(new_snapshots)\n",
    "\n",
    "    # tst = tst + 1\n",
    "    # if tst == 4:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_time_step = datetime(year=2023, month=9, day=1)\n",
    "\n",
    "db_params = {\n",
    "    'database': environ.get(\"POSTGRES_DB\"),\n",
    "    'user': environ.get(\"POSTGRES_USER\"),\n",
    "    'password': environ.get(\"POSTGRES_PASSWORD\"),\n",
    "    'host': environ.get(\"DB_HOST\"),\n",
    "    'port': environ.get(\"DB_PORT\"),\n",
    "}\n",
    "env_config = {\n",
    "    \"db_params\": db_params, \n",
    "    \"max_episode_steps\": 500, \n",
    "    \"the_current_time_step\": starting_time_step,\n",
    "    \"print_output\": False,\n",
    "    \"is_test\": False,\n",
    "    \"test_steps\": 200,\n",
    "    \"n_step_stocks\": 5,\n",
    "    \"n_step_cmmdties\": 5,\n",
    "    \"preparing\": True\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_current_time_step = env_config.get(\"the_current_time_step\")\n",
    "max_epi_len = 30\n",
    "\n",
    "__last_time_step = the_current_time_step + relativedelta(days=max_epi_len)\n",
    "the_current_time_step = pytz.utc.localize(datetime.strptime(str(the_current_time_step), '%Y-%m-%d %H:%M:%S'))\n",
    "__last_time_step = pytz.utc.localize(datetime.strptime(str(__last_time_step), '%Y-%m-%d %H:%M:%S'))\n",
    "str_time_step = str(the_current_time_step)\n",
    "\n",
    "str_last_time_step = str(__last_time_step)\n",
    "db_params = env_config.get(\"db_params\")\n",
    "db_conn = psycopg2.connect(**db_params)\n",
    "cursor = db_conn.cursor()\n",
    "\n",
    "stcks_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM stocks\n",
    "\"\"\"\n",
    "cursor.execute(stcks_query)\n",
    "stck_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "stck_df = pd.DataFrame(stck_data, columns=column_names) \n",
    "# stck_df = stck_df.to_dict(orient='records')\n",
    "\n",
    "stcks_buffer_query = f\"\"\"\n",
    "    SELECT stocks_buffers.*, stocks.index\n",
    "    FROM stocks_buffers \n",
    "    JOIN stocks on stocks.id = stocks_buffers.stock_id\n",
    "    WHERE \n",
    "    captured_at >= '{str_time_step}' AND captured_at <= '{str_last_time_step}'\n",
    "    AND stocks.index = 4\n",
    "\"\"\"\n",
    "cursor.execute(stcks_buffer_query)\n",
    "stcks_buffer_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "stcks_buffer_df = pd.DataFrame(stcks_buffer_data, columns=column_names)\n",
    "\n",
    "# stcks_buffer_df = stcks_buffer_df.to_dict(orient='records')\n",
    "\n",
    "cmmdties_buffer_query = f\"\"\"\n",
    "    SELECT commodities_buffers.*, commodities.index\n",
    "    FROM commodities_buffers \n",
    "    JOIN commodities on commodities.id = commodities_buffers.commodity_id\n",
    "    WHERE \n",
    "    captured_at >= '{str_time_step}' AND captured_at <= '{str_last_time_step}'\n",
    "\"\"\"\n",
    "cursor.execute(cmmdties_buffer_query)\n",
    "cmmdties_buffer_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "cmmdties_buffer_df = pd.DataFrame(cmmdties_buffer_data, columns=column_names)\n",
    "# cmmdties_buffer_df = cmmdties_buffer_df.to_dict(orient='records')\n",
    "\n",
    "cmmdties_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM commodities\n",
    "\"\"\"\n",
    "cursor.execute(cmmdties_query)\n",
    "cmmdties_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "cmmdties_df = pd.DataFrame(cmmdties_data, columns=column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify the date range\n",
    "start_date = the_current_time_step\n",
    "end_date = the_current_time_step + relativedelta(days=max_epi_len)\n",
    "\n",
    "# Create a list of working days within the date range\n",
    "working_days = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "\n",
    "# Repeat rows for each index from 1 to 100\n",
    "index_range = range(4, 5)\n",
    "\n",
    "# Create a DataFrame with the specified structure\n",
    "stcks_data = {'captured_at': [], 'change': [], 'index': []}\n",
    "\n",
    "for date in working_days:\n",
    "    for idx in index_range:\n",
    "        stcks_data['captured_at'].append(date)\n",
    "        stcks_data['change'].append(0)\n",
    "        stcks_data['index'].append(idx)\n",
    "\n",
    "stcks_working_day_df = pd.DataFrame(stcks_data)\n",
    "\n",
    "cmmdts_data = {'captured_at': [], 'index': []}\n",
    "\n",
    "for date in working_days:\n",
    "    for idx in index_range:\n",
    "        cmmdts_data['captured_at'].append(date)\n",
    "        cmmdts_data['index'].append(idx)\n",
    "\n",
    "cmmdts_working_day_df = pd.DataFrame(cmmdts_data)\n",
    "\n",
    "# Print the resulting DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(stcks_buffer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id\tcaptured_at\tprice_snapshot\tchange\tvolume\tbid_vol\tbid_price\toffer_vol\toffer_price\tstock_id\tclose\thigh\tlow\topen\tindex\n",
    "\n",
    "stocks_fields_list = [\"id\", \"price_snapshot\", \"volume\", \"bid_vol\",\t\"bid_price\", \"offer_vol\",\t\"offer_price\",\t\"stock_id\",\t\"close\",\t\"high\",\t\"low\",\t\"open\",]\n",
    "\n",
    "stcks_df1 = stcks_buffer_df\n",
    "stcks_df2 = stcks_working_day_df\n",
    "\n",
    "# Merge dataframes on 'captured_at' and 'index' and keep only the rows present in stcks_df2\n",
    "stcks_merged_df = pd.merge(stcks_df2, stcks_df1, on=['captured_at', 'index'], how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "stcks_concatenated_df = pd.concat([stcks_df1, stcks_merged_df], ignore_index=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "\n",
    "# Sort the DataFrame by 'index' and 'captured_at'\n",
    "stcks_concatenated_df = stcks_concatenated_df.sort_values(by=['index', 'captured_at'])\n",
    "\n",
    "# Forward fill NaN values based on 'index'\n",
    "\n",
    "for field in stocks_fields_list:\n",
    "    stcks_concatenated_df[field] = stcks_concatenated_df.groupby('index')[field].fillna(method='ffill')\n",
    "    stcks_concatenated_df[field] = stcks_concatenated_df.groupby('index')[field].fillna(method='bfill')\n",
    "\n",
    "\n",
    "# Display the upcaptured_atd DataFrame\n",
    "\n",
    "stcks_concatenated_df['change'].fillna(stcks_concatenated_df['change_x'], inplace=True)\n",
    "\n",
    "# Drop the 'change_x' column if you no longer need it\n",
    "stcks_concatenated_df.drop(columns=['change_x', 'change_y'], inplace=True)\n",
    "display(stcks_concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id\tcaptured_at\tprice_snapshot\tchange\tvolume\tbid_vol\tbid_price\toffer_vol\toffer_price\tstock_id\tclose\thigh\tlow\topen\tindex\n",
    "\n",
    "commodities_fields_list = [\"id\", \"price_snapshot\", \"commodity_id\"]\n",
    "\n",
    "cmmdts_df1 = cmmdties_buffer_df\n",
    "cmmdts_df2 = cmmdts_working_day_df\n",
    "\n",
    "# Merge dataframes on 'captured_at' and 'index' and keep only the rows present in cmmdts_df2\n",
    "cmmdts_merged_df = pd.merge(cmmdts_df2, cmmdts_df1, on=['captured_at', 'index'], how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "cmmdts_concatenated_df = pd.concat([cmmdts_df1, cmmdts_merged_df], ignore_index=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "\n",
    "# Sort the DataFrame by 'index' and 'captured_at'\n",
    "cmmdts_concatenated_df = cmmdts_concatenated_df.sort_values(by=['index', 'captured_at'])\n",
    "\n",
    "# Forward fill NaN values based on 'index'\n",
    "\n",
    "for field in commodities_fields_list:\n",
    "    cmmdts_concatenated_df[field] = cmmdts_concatenated_df.groupby('index')[field].fillna(method='ffill')\n",
    "    cmmdts_concatenated_df[field] = cmmdts_concatenated_df.groupby('index')[field].fillna(method='bfill')\n",
    "\n",
    "\n",
    "# Display the upcaptured_atd DataFrame\n",
    "\n",
    "display(cmmdts_concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for illustration\n",
    "data1 = {'date': ['2023-01-01', '2023-01-06', '2023-01-02', '2023-01-03'],\n",
    "         'id': [1, 1, 2, 3],\n",
    "         'value': ['A', \"Yo\", 'B', 'C'],\n",
    "         'valuea': ['a', \"y\", 'b', 'c'],\n",
    "         'change': [1,2,3,4],\n",
    "         }\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'date': ['2023-01-01', '2023-01-04', '2023-01-04', '2023-01-03'],\n",
    "         'id': [1, 1, 2, 3],\n",
    "         'change': [0,0,0,0],\n",
    "         }\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge dataframes on 'date' and 'id' and keep only the rows present in df2\n",
    "merged_df = pd.merge(df2, df1, on=['date', 'id'], how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([df1, merged_df], ignore_index=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "\n",
    "# Sort the DataFrame by 'id' and 'date'\n",
    "concatenated_df = concatenated_df.sort_values(by=['id', 'date'])\n",
    "\n",
    "# Forward fill NaN values based on 'id'\n",
    "concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='ffill')\n",
    "concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='bfill')\n",
    "\n",
    "concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='ffill')\n",
    "concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='bfill')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "\n",
    "concatenated_df['change'].fillna(concatenated_df['change_x'], inplace=True)\n",
    "\n",
    "# Drop the 'change_x' column if you no longer need it\n",
    "concatenated_df.drop(columns=['change_x', 'change_y'], inplace=True)\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dyIndx in range(1,3):\n",
    "    print(dyIndx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from = date(2016, 1, 1)\n",
    "end_date = date(2017, 1, 1)\n",
    "data = stocks(\"UBL\", start=start_from, end=end_date)\n",
    "display(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2env",
   "language": "python",
   "name": "v2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
