{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import django\n",
    "import os\n",
    "file_dir = \"/Users/mirbilal/Desktop/MobCommission/commissionV2/\"\n",
    "if file_dir not in sys.path:\n",
    "    sys.path.insert(0, file_dir)\n",
    "\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"commissionerv2.settings\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\" \n",
    "django.setup()\n",
    "from datetime import datetime, date\n",
    "from psx import stocks, tickers\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import investpy\n",
    "import yfinance as yf\n",
    "from matplotlib.dates import relativedelta\n",
    "from apps.environment.models import Stock\n",
    "from apps.environment.models.stock import StockBuffer\n",
    "from apps.environment.models.commodity import Commodity\n",
    "from apps.environment.models.commodity import CommodityBuffer\n",
    "import pytz\n",
    "from os import environ\n",
    "from math import ceil, floor\n",
    "import psycopg2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tickers is your DataFrame\n",
    "\n",
    "\n",
    "tickers_column = tickers()\n",
    "# display(tickers_column)\n",
    "symbols = list(tickers_column[\"symbol\"])\n",
    "names = list(tickers_column[\"name\"])\n",
    "len_syms = len(symbols)\n",
    "# # print(symbols)\n",
    "hundred_inds = pd.read_csv('KSE100 indx Comps - stock-exchange-kse-100pakistan.csv').to_dict(orient='records')\n",
    "# print(hundred_inds)\n",
    "hun_syms = {}\n",
    "\n",
    "all_syms = {}\n",
    "for ind in range(len_syms):\n",
    "    all_syms[symbols[ind]] = names[ind]\n",
    "\n",
    "the_index = 1\n",
    "for hun_ind in hundred_inds:\n",
    "    if hun_ind[\"SYMBOL\"] in all_syms:\n",
    "        sym_b = hun_ind[\"SYMBOL\"]\n",
    "        if sym_b not in hun_syms:\n",
    "            hun_syms[sym_b] = {\n",
    "                \"index\": the_index,\n",
    "                \"name\": all_syms[sym_b]\n",
    "            }\n",
    "            the_index = the_index + 1\n",
    "\n",
    "db_stcks = {}\n",
    "all_stcks = Stock.objects.all()\n",
    "max_indx = len(all_stcks)\n",
    "for astck in all_stcks:\n",
    "    db_stcks[astck.symbol] = astck.symbol\n",
    "\n",
    "new_stcks_list = []\n",
    "for sym_bl in hun_syms:\n",
    "    if sym_bl not in db_stcks:\n",
    "        sym_data = hun_syms[sym_bl]\n",
    "        new_indx = sym_data[\"index\"]\n",
    "        if new_indx<=max_indx:\n",
    "            new_indx = max_indx+1\n",
    "        max_indx = max_indx+1\n",
    "        new_stcks_list.append(\n",
    "            Stock(\n",
    "                index=sym_data[\"index\"],\n",
    "                symbol=sym_bl,\n",
    "                name=sym_data[\"name\"],\n",
    "                type=100\n",
    "            )\n",
    "        )\n",
    "\n",
    "Stock.objects.bulk_create(new_stcks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_stocks = Stock.objects.all()\n",
    "\n",
    "stock_syms = {}\n",
    "\n",
    "for a_stock in all_stocks:\n",
    "    if a_stock.symbol not in stock_syms:\n",
    "        stock_syms[a_stock.symbol] = {\n",
    "            \"object\": a_stock,\n",
    "            \"snapshots\": {},\n",
    "            \"last_date\": None\n",
    "        }\n",
    "\n",
    "snapshots = StockBuffer.objects.select_related(\"stock\").all()\n",
    "for snapshot in snapshots:\n",
    "    snp_stck = snapshot.stock\n",
    "    snp_stck_symbl = snp_stck.symbol\n",
    "    if snp_stck_symbl in stock_syms:\n",
    "        snp_data = stock_syms[snp_stck_symbl]\n",
    "        stck_snps = snp_data[\"snapshots\"]\n",
    "        if snapshot.captured_at not in stck_snps:\n",
    "            stck_snps[snapshot.captured_at.strftime('%Y-%m-%d %H:%M:%S')] = snapshot\n",
    "            if not snp_data[\"last_date\"]:\n",
    "                snp_data[\"last_date\"] = snapshot.captured_at.date()\n",
    "\n",
    "# tst = 1\n",
    "for stck_sym, stck_data in stock_syms.items():\n",
    "    new_snapshots = []\n",
    "    last_date: datetime = stck_data[\"last_date\"]\n",
    "    if last_date is None:\n",
    "        start_from = date(2020, 1, 1)\n",
    "    else:\n",
    "        start_from = last_date\n",
    "    print(last_date, stck_sym, start_from)\n",
    "    end_date = pytz.utc.localize(datetime.now() + relativedelta(days=2)).date()\n",
    "    data = stocks(stck_sym, start=start_from, end=end_date)\n",
    "    data_points_len = len(data)\n",
    "    stck_snpshts = stck_data[\"snapshots\"]\n",
    "    the_stock: Stock = stck_data[\"object\"]\n",
    "    last_data_point = None\n",
    "    for digi in range(data_points_len):\n",
    "        data_point = data.iloc[digi]\n",
    "        time_point = data_point.name.to_pydatetime()\n",
    "        time_point_str = data_point.name.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if time_point_str not in stck_snpshts:\n",
    "            zone_point_time = pytz.utc.localize(time_point)\n",
    "            nw_price = data_point.Close\n",
    "            if last_data_point is None:\n",
    "                change = 0\n",
    "            else:\n",
    "                old_price = last_data_point.Close\n",
    "                if old_price and old_price > 0:\n",
    "                    change = (nw_price-old_price)/old_price\n",
    "                else:\n",
    "                    change = 0\n",
    "            new_snapshots.append(StockBuffer(\n",
    "                stock = the_stock,\n",
    "                captured_at = zone_point_time,\n",
    "                price_snapshot = nw_price,\n",
    "                change = change,\n",
    "                volume = data_point.Volume,\n",
    "                bid_vol = 100000000,\n",
    "                bid_price = nw_price,\n",
    "                offer_vol = 100000000,\n",
    "                offer_price = nw_price,\n",
    "                open = data_point.Open,\n",
    "                close = nw_price,\n",
    "                high = data_point.High,\n",
    "                low = data_point.Low\n",
    "            ))\n",
    "        last_data_point = data_point\n",
    "    StockBuffer.objects.bulk_create(new_snapshots)\n",
    "\n",
    "    # tst = tst + 1\n",
    "    # if tst == 4:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_commodities = Commodity.objects.all()\n",
    "\n",
    "commodity_syms = {}\n",
    "\n",
    "for a_commodity in all_commodities:\n",
    "    if a_commodity.symbol not in commodity_syms:\n",
    "        commodity_syms[a_commodity.symbol] = {\n",
    "            \"object\": a_commodity,\n",
    "            \"snapshots\": {},\n",
    "            \"last_date\": None\n",
    "        }\n",
    "\n",
    "snapshots = CommodityBuffer.objects.select_related(\"commodity\").all()\n",
    "for snapshot in snapshots:\n",
    "    snp_cmmdty = snapshot.commodity\n",
    "    snp_cmmdty_symbl = snp_cmmdty.symbol\n",
    "    if snp_cmmdty_symbl in commodity_syms:\n",
    "        cmmdty_data = commodity_syms[snp_cmmdty_symbl]\n",
    "        cmmdty_snps = cmmdty_data[\"snapshots\"]\n",
    "        if snapshot.captured_at not in cmmdty_snps:\n",
    "            cmmdty_snps[snapshot.captured_at.strftime('%Y-%m-%d %H:%M:%S')] = snapshot\n",
    "            if not cmmdty_data[\"last_date\"]:\n",
    "                cmmdty_data[\"last_date\"] = snapshot.captured_at.date()\n",
    "\n",
    "# tst = 1\n",
    "for cmmdty_sym, cmmdty_data in commodity_syms.items():\n",
    "    new_snapshots = []\n",
    "    last_date: datetime = cmmdty_data[\"last_date\"]\n",
    "    if last_date is None:\n",
    "        start_from = date(2019, 12, 30)\n",
    "    else:\n",
    "        start_from = date(2019, 12, 30)\n",
    "        # start_from = last_date\n",
    "    print(last_date, cmmdty_sym, start_from)\n",
    "    end_date = pytz.utc.localize(datetime.now() + relativedelta(days=2)).date()\n",
    "    data = yf.download(cmmdty_sym, start=start_from, end=end_date)\n",
    "    data_points_len = len(data)\n",
    "    cmmdty_snpshts = cmmdty_data[\"snapshots\"]\n",
    "    the_commodity: Commodity = cmmdty_data[\"object\"]\n",
    "    last_data_point = None\n",
    "    for digi in range(data_points_len):\n",
    "        data_point = data.iloc[digi]\n",
    "        time_point = data_point.name.to_pydatetime()\n",
    "        time_point_str = data_point.name.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if time_point_str not in cmmdty_snpshts:\n",
    "            zone_point_time = pytz.utc.localize(time_point)\n",
    "            nw_price = data_point.Close\n",
    "            if last_data_point is None:\n",
    "                change = 0\n",
    "            else:\n",
    "                old_price = last_data_point.Close\n",
    "                if old_price and old_price > 0:\n",
    "                    change = (nw_price-old_price)/old_price\n",
    "                else:\n",
    "                    change = 0\n",
    "            new_snapshots.append(CommodityBuffer(\n",
    "                commodity = the_commodity,\n",
    "                captured_at = zone_point_time,\n",
    "                price_snapshot = nw_price,\n",
    "            ))\n",
    "        last_data_point = data_point\n",
    "    CommodityBuffer.objects.bulk_create(new_snapshots)\n",
    "\n",
    "    # tst = tst + 1\n",
    "    # if tst == 4:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_time_step = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "db_params = {\n",
    "    'database': environ.get(\"POSTGRES_DB\"),\n",
    "    'user': environ.get(\"POSTGRES_USER\"),\n",
    "    'password': environ.get(\"POSTGRES_PASSWORD\"),\n",
    "    'host': environ.get(\"DB_HOST\"),\n",
    "    'port': environ.get(\"DB_PORT\"),\n",
    "}\n",
    "env_config = {\n",
    "    \"db_params\": db_params, \n",
    "    \"max_episode_steps\": 1000, \n",
    "    \"the_current_time_step\": starting_time_step,\n",
    "    \"print_output\": False,\n",
    "    \"is_test\": False,\n",
    "    \"test_steps\": 200,\n",
    "    \"n_step_stocks\": 5,\n",
    "    \"n_step_cmmdties\": 5,\n",
    "    \"preparing\": True\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_current_time_step = env_config.get(\"the_current_time_step\")\n",
    "max_epi_len = 50\n",
    "\n",
    "__last_time_step = the_current_time_step + relativedelta(days=max_epi_len)\n",
    "the_current_time_step = pytz.utc.localize(datetime.strptime(str(the_current_time_step), '%Y-%m-%d %H:%M:%S'))\n",
    "__last_time_step = pytz.utc.localize(datetime.strptime(str(__last_time_step), '%Y-%m-%d %H:%M:%S'))\n",
    "str_time_step = str(the_current_time_step)\n",
    "\n",
    "str_last_time_step = str(__last_time_step)\n",
    "db_params = env_config.get(\"db_params\")\n",
    "db_conn = psycopg2.connect(**db_params)\n",
    "cursor = db_conn.cursor()\n",
    "\n",
    "stcks_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM stocks\n",
    "\"\"\"\n",
    "cursor.execute(stcks_query)\n",
    "stck_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "stck_df = pd.DataFrame(stck_data, columns=column_names) \n",
    "# stck_df = stck_df.to_dict(orient='records')\n",
    "\n",
    "stcks_buffer_query = f\"\"\"\n",
    "    SELECT stocks_buffers.*, stocks.index\n",
    "    FROM stocks_buffers \n",
    "    JOIN stocks on stocks.id = stocks_buffers.stock_id\n",
    "    WHERE \n",
    "    captured_at >= '{str_time_step}' AND captured_at <= '{str_last_time_step}'\n",
    "\"\"\"\n",
    "cursor.execute(stcks_buffer_query)\n",
    "stcks_buffer_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "stcks_buffer_df = pd.DataFrame(stcks_buffer_data, columns=column_names)\n",
    "\n",
    "# stcks_buffer_df = stcks_buffer_df.to_dict(orient='records')\n",
    "\n",
    "cmmdties_buffer_query = f\"\"\"\n",
    "    SELECT commodities_buffers.*, commodities.index\n",
    "    FROM commodities_buffers \n",
    "    JOIN commodities on commodities.id = commodities_buffers.commodity_id\n",
    "    WHERE \n",
    "    captured_at >= '{str_time_step}' AND captured_at <= '{str_last_time_step}'\n",
    "\"\"\"\n",
    "cursor.execute(cmmdties_buffer_query)\n",
    "cmmdties_buffer_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "cmmdties_buffer_df = pd.DataFrame(cmmdties_buffer_data, columns=column_names)\n",
    "# cmmdties_buffer_df = cmmdties_buffer_df.to_dict(orient='records')\n",
    "\n",
    "cmmdties_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM commodities\n",
    "\"\"\"\n",
    "cursor.execute(cmmdties_query)\n",
    "cmmdties_data = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "cmmdties_df = pd.DataFrame(cmmdties_data, columns=column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_buffers_list = list(StockBuffer.objects.all().filter(captured_at__lte = '2020-05-01'))\n",
    "\n",
    "# Convert the list of StockBuffer instances to a DataFrame\n",
    "# Convert the list of StockBuffer instances to a DataFrame\n",
    "df = pd.DataFrame.from_records([stock_buffer.__dict__ for stock_buffer in stock_buffers_list])\n",
    "\n",
    "# Convert 'captured_at' column to datetime and localize to UTC\n",
    "df = pd.DataFrame.from_records([stock_buffer.__dict__ for stock_buffer in stock_buffers_list])\n",
    "\n",
    "# Convert 'captured_at' column to datetime (if not already) and keep it timezone-aware\n",
    "df['captured_at'] = pd.to_datetime(df['captured_at'])\n",
    "\n",
    "# Get unique working days present in the data\n",
    "unique_working_days = df['captured_at'].dt.date.unique()\n",
    "\n",
    "# Create a DataFrame to store the final result\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each stock and working day combination\n",
    "for stock_id in df['stock_id'].unique():\n",
    "    stock_data = df[df['stock_id'] == stock_id]\n",
    "\n",
    "    for working_day in unique_working_days:\n",
    "        working_day_timestamp = pd.Timestamp(working_day, tz='UTC')\n",
    "        if not any(stock_data['captured_at'].dt.floor('D').eq(working_day_timestamp)):\n",
    "            # If data is missing for the working day, use the most recent data for that stock\n",
    "            most_recent_data = stock_data[stock_data['captured_at'] < working_day_timestamp].sort_values(by='captured_at', ascending=False).head(1)\n",
    "            if not most_recent_data.empty:\n",
    "                most_recent_data['captured_at'] = working_day_timestamp\n",
    "                result_df = pd.concat([result_df, most_recent_data], ignore_index=True)\n",
    "\n",
    "# Drop duplicate rows based on 'stock_id' and 'captured_at'\n",
    "result_df = result_df.drop_duplicates(subset=['stock_id', 'captured_at'])\n",
    "\n",
    "# Sort the DataFrame by 'stock_id' and 'captured_at'\n",
    "result_df = result_df.sort_values(by=['stock_id', 'captured_at'])\n",
    "\n",
    "# Reset the index\n",
    "result_df = result_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(stcks_buffer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stcks_buffer_df['captured_at'] = pd.to_datetime(stcks_buffer_df['captured_at'])\n",
    "\n",
    "# Set 'stock_id' and 'captured_at' as the index\n",
    "stcks_buffer_df.set_index(['stock_id', 'captured_at'], inplace=True)\n",
    "\n",
    "# Resample the data to fill in the missing days\n",
    "resampled_df = stcks_buffer_df.groupby(level='stock_id').resample('D').ffill()\n",
    "\n",
    "# Reset the index to make 'stock_id' and 'captured_at' regular columns\n",
    "resampled_df.reset_index(inplace=True)\n",
    "\n",
    "# Ensure that the 'captured_at' column is in datetime format\n",
    "stcks_buffer_df['captured_at'] = pd.to_datetime(stcks_buffer_df['captured_at'])\n",
    "\n",
    "# Create a list to store the final DataFrames\n",
    "result_datasets = []\n",
    "\n",
    "# Group by stock_id and iterate over each group\n",
    "for stock_id, group in stcks_buffer_df.groupby('stock_id'):\n",
    "    # Sort the group by 'captured_at' to ensure it's ordered\n",
    "    group = group.sort_values(by='captured_at')\n",
    "\n",
    "    # Fill missing values with the previous day's values\n",
    "    group_filled = group.fillna(method='ffill')\n",
    "\n",
    "    # Fill remaining missing values with the next day's values\n",
    "    group_filled = group_filled.fillna(method='bfill')\n",
    "\n",
    "    # Append the filled group to the result_datasets list\n",
    "    result_datasets.append(group_filled)\n",
    "\n",
    "# Concatenate the datasets into a single DataFrame\n",
    "final_stocks_buffer_df = pd.concat(result_datasets)\n",
    "\n",
    "\n",
    "print(len(final_stocks_buffer_df))\n",
    "display(final_stocks_buffer_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ind_left</th>\n",
       "      <th>ind_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>David</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Name ind_left ind_right\n",
       "0   1    Alice        a         a\n",
       "1   2      Bob        b         b\n",
       "2   3  Charlie        c         c\n",
       "3   4    David        d         d\n",
       "4   5      NaN      NaN         b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ind_left</th>\n",
       "      <th>ind_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>David</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Name ind_left ind_right\n",
       "0   1    Alice        a         a\n",
       "1   2      Bob        b         b\n",
       "2   3  Charlie        c         c\n",
       "3   4    David        d         d\n",
       "4   5      NaN      NaN         b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data for two datasets\n",
    "import numpy as np\n",
    "data1 = {'ID': [1, 2, 3, 4],\n",
    "         'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "         'ind': ['a', 'b', 'c', 'd']\n",
    "         }\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'ID': [1, 2, 3, 4, 5],\n",
    "         'Name': [np.nan, np.nan, 'x', np.nan, np.nan],\n",
    "         'ind': ['a', 'b', 'c', 'd', 'b']\n",
    "         }\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge based on the 'ID' column\n",
    "merged_df = pd.merge(df1, df2, on='ID', how=\"outer\", suffixes=('_left', '_right'))\n",
    "display(merged_df)\n",
    "merged_df['Name_left'] = merged_df['Name_left'].fillna(merged_df.groupby(['ind_left', 'ind_right'])['Name_left'].ffill().bfill())\n",
    "display(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name ind  Name_left ind_left\n",
      "0   1    Alice   a        NaN      NaN\n",
      "1   2      Bob   b        NaN      NaN\n",
      "2   3  Charlie   c        NaN      NaN\n",
      "3   4    David   d        NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data1 = {'ID': [1, 2, 3, 4],\n",
    "         'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "         'ind': ['a', 'b', 'c', 'd']}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'ID': [1, 2, 3, 4, 5],\n",
    "         'Name': [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "         'ind': ['a', 'b', 'c', 'd', 'b']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Filter rows where 'Name_left' is NaN\n",
    "df_left_missing = df1[df1['Name'].isna()]\n",
    "\n",
    "# Merge all missing rows with 'Name_right' not NaN\n",
    "merged_df = pd.merge(df_left_missing, df2, on='ID', how='left', suffixes=('_left', '_right'))\n",
    "\n",
    "# Use 'Name_right' to fill 'Name_left'\n",
    "merged_df['Name_left'] = merged_df['Name_right']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df = merged_df[['ID', 'Name_left', 'ind_left']]\n",
    "\n",
    "# Concatenate with the original dataframe, including all missing rows from df2\n",
    "result_df = pd.concat([df1, merged_df, df2[df2['Name'].notna()]]).sort_values(by='ID').reset_index(drop=True)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/199389832.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  dfs = {k:v for k,v in total_merge.groupby('_merge')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_x</th>\n",
       "      <th>id</th>\n",
       "      <th>value_left</th>\n",
       "      <th>date_y</th>\n",
       "      <th>value_right</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>X</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>Y</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>Z</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_x  id value_left      date_y value_right _merge\n",
       "0  2023-01-01   1          A  2023-01-01           X   both\n",
       "1  2023-01-02   2          B  2023-01-02           Y   both\n",
       "2  2023-01-03   3          C  2023-01-04           Z   both"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample data for illustration\n",
    "data1 = {'date': ['2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "         'id': [1, 2, 3],\n",
    "         'value_left': ['A', 'B', 'C']}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'date': ['2023-01-01', '2023-01-02', '2023-01-04'],\n",
    "         'id': [1, 2, 3],\n",
    "         'value_right': ['X', 'Y', 'Z']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge dataframes based on 'date' and 'id'\n",
    "total_merge = df1.merge(df2, on='id', how='outer', indicator=True)\n",
    "dfs = {k:v for k,v in total_merge.groupby('_merge')}\n",
    "dfs['both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df          date  id value_right value_left\n",
      "1  2023-01-04   2         NaN        NaN\n",
      "         date  id value_left\n",
      "1  2023-01-04   2        NaN\n",
      "         date  id value_left\n",
      "0  2023-01-01   1          A\n",
      "1  2023-01-02   2          B\n",
      "2  2023-01-03   3          C\n",
      "3  2023-01-04   2        NaN\n",
      "         date  id value_left\n",
      "0  2023-01-01   1          A\n",
      "1  2023-01-02   2          B\n",
      "3  2023-01-04   2          B\n",
      "2  2023-01-03   3          C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1115710762.py:29: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['value_left'] = concatenated_df.groupby('id')['value_left'].fillna(method='ffill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1115710762.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['value_left'] = concatenated_df.groupby('id')['value_left'].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Sample data for illustration\n",
    "data1 = {'date': ['2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "         'id': [1, 2, 3],\n",
    "         'value_left': ['A', 'B', 'C']}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'date': ['2023-01-01', '2023-01-04', '2023-01-03'],\n",
    "         'id': [1, 2, 3],\n",
    "         'value_right': ['X',  np.nan, np.nan]}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge dataframes on 'date' and 'id' and keep only the rows present in df2\n",
    "merged_df = pd.merge(df2, df1, on=['date', 'id'], how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "print(\"merged_df\", merged_df)\n",
    "merged_df =merged_df.drop('value_left', axis=1)\n",
    "merged_df = merged_df.rename(columns={'value_right': 'value_left'})\n",
    "\n",
    "print(merged_df)\n",
    "\n",
    "concatenated_df = pd.concat([df1, merged_df], ignore_index=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(concatenated_df)\n",
    "\n",
    "# Sort the DataFrame by 'id' and 'date'\n",
    "concatenated_df = concatenated_df.sort_values(by=['id', 'date'])\n",
    "\n",
    "# Forward fill NaN values based on 'id'\n",
    "concatenated_df['value_left'] = concatenated_df.groupby('id')['value_left'].fillna(method='ffill')\n",
    "\n",
    "# Backward fill remaining NaN values based on 'id'\n",
    "concatenated_df['value_left'] = concatenated_df.groupby('id')['value_left'].fillna(method='bfill')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(concatenated_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  id value valuea\n",
      "0  2023-01-01   1     A      a\n",
      "3  2023-01-04   1     A      a\n",
      "1  2023-01-02   2     B      b\n",
      "4  2023-01-04   2     B      b\n",
      "2  2023-01-03   3     C      c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/3868119292.py:29: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='ffill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/3868119292.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='bfill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/3868119292.py:33: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='ffill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/3868119292.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Sample data for illustration\n",
    "data1 = {'date': ['2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "         'id': [1, 2, 3],\n",
    "         'value': ['A', 'B', 'C'],\n",
    "         'valuea': ['a', 'b', 'c']}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'date': ['2023-01-01', '2023-01-04', '2023-01-04', '2023-01-03'],\n",
    "         'id': [1, 1, 2, 3],\n",
    "         'value': ['X', np.nan,  np.nan, np.nan,],\n",
    "         'valuea': [np.nan, np.nan, np.nan, np.nan]}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge dataframes on 'date' and 'id' and keep only the rows present in df2\n",
    "merged_df = pd.merge(df2, df1, on=['date', 'id'], how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "merged_df =merged_df.drop(['value_x', \"valuea_x\", \"valuea_y\"], axis=1)\n",
    "# merged_df =merged_df.drop('valuea_x', axis=1)\n",
    "merged_df = merged_df.rename(columns={'value_y': 'value', \"valuea_y\": \"valuea\"})\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([df1, merged_df], ignore_index=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "\n",
    "# Sort the DataFrame by 'id' and 'date'\n",
    "concatenated_df = concatenated_df.sort_values(by=['id', 'date'])\n",
    "\n",
    "# Forward fill NaN values based on 'id'\n",
    "concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='ffill')\n",
    "concatenated_df['value'] = concatenated_df.groupby('id')['value'].fillna(method='bfill')\n",
    "\n",
    "concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='ffill')\n",
    "concatenated_df['valuea'] = concatenated_df.groupby('id')['valuea'].fillna(method='bfill')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  id value_left  other_column1  other_column2\n",
      "0  2023-01-01   1          A           10.0          100.0\n",
      "1  2023-01-02   2          B           20.0          200.0\n",
      "3  2023-01-04   2          B           20.0          200.0\n",
      "2  2023-01-03   3          C           30.0          300.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1853133386.py:15: FutureWarning: DataFrameGroupBy.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='ffill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1853133386.py:15: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='ffill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1853133386.py:18: FutureWarning: DataFrameGroupBy.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='bfill')\n",
      "/var/folders/ct/fjh_5m0n2zj9r5lpl1nrd20m0000gn/T/ipykernel_48833/1853133386.py:18: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your dataset with additional columns\n",
    "data = {'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\n",
    "        'id': [1, 2, 3, 2],\n",
    "        'value_left': ['A', 'B', 'C', None],\n",
    "        'other_column1': [10, 20, 30, None],\n",
    "        'other_column2': [100, 200, 300, None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by 'id' and 'date'\n",
    "df = df.sort_values(by=['id', 'date'])\n",
    "\n",
    "# Forward fill NaN values based on 'id' for specific columns\n",
    "columns_to_fill = ['value_left', 'other_column1', 'other_column2']\n",
    "df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='ffill')\n",
    "\n",
    "# Backward fill remaining NaN values based on 'id' for specific columns\n",
    "df[columns_to_fill] = df.groupby('id')[columns_to_fill].fillna(method='bfill')\n",
    "\n",
    "# Fill remaining NaN values in other columns with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2env",
   "language": "python",
   "name": "v2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
